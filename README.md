# SmartClause - AI-Powered Legal Document Analysis Platform

**ğŸš€ MVP Ready!** SmartClause is a complete AI-powered legal document analysis platform focused on the Russian legal market. The platform leverages RAG (Retrieval-Augmented Generation) technology with legal vector databases to provide intelligent document analysis and interactive consultation capabilities.

## ğŸ¯ What You Can Do

**Ready to use out of the box:**
- **ğŸ“„ Upload and Analyze Documents**: Upload legal documents (up to 10MB) and get AI-powered risk analysis and recommendations
- **ğŸ” Semantic Legal Search**: Search through a comprehensive Civil Code database using natural language queries
- **ğŸ’» Interactive Web Interface**: Complete workflow from document upload to detailed analysis results
- **âš¡ Real-time Processing**: Watch your documents being processed with live status updates
- **ğŸ”— REST API Access**: Full programmatic access to all analysis capabilities

## âœ¨ Key Features

### ğŸ“‹ Document Analysis Pipeline
- **Smart Upload Interface**: Drag-and-drop document upload with progress tracking
- **AI-Powered Analysis**: Legal document analysis for risks, compliance issues, and recommendations
- **Comprehensive Results**: Detailed analysis with causes, risks, and actionable recommendations
- **Multiple File Formats**: Support for text files and structured documents

### ğŸ“š Legal Knowledge Base
- **Chunked Civil Code Database**: Complete Russian Civil Code with 190,000+ rules and 413,000+ text chunks with embeddings
- **Semantic Search**: Vector-based similarity search using BAAI/bge-m3 embeddings on text chunks
- **Configurable Retrieval**: Multiple distance functions (cosine, L2, inner product)
- **Structured Metadata**: Articles organized by sections, chapters, and legal references with precise chunk positioning

### ğŸ› ï¸ Technology Stack
- **Frontend**: Vue.js 3 with modern UI components and routing
- **Backend**: Spring Boot REST API with Swagger documentation
- **AI Engine**: FastAPI microservice with LangChain and OpenRouter integration
- **Database**: PostgreSQL with pgvector extension for vector operations
- **LLM Integration**: Google Gemini 2.5 Flash via OpenRouter for analysis generation
- **Embeddings**: BAAI/bge-m3 sentence transformer for semantic understanding
- **Deployment**: Docker Compose orchestration for easy setup

## ğŸš€ Quick Start

### Prerequisites
- **Docker and Docker Compose** (required)
- **Python 3.8+** (for dataset download script)
- **OpenRouter API Key** ([Get one here](https://openrouter.ai/))
- **Internet Connection** (for downloading datasets from Hugging Face)

### 1. Clone Repository

```bash
# Clone the repository
git clone https://github.com/your-username/SmartClause.git
cd SmartClause
```

### 2. Download Required Datasets
```bash
# Download datasets from Hugging Face Hub (~1.2GB total)
python analyzer/scripts/download_datasets.py

# Or force re-download if files exist
python analyzer/scripts/download_datasets.py --force
```

**ğŸ“Š Dataset Information:**
- **Source:** [Hugging Face Hub - narly/russian-codexes-bge-m3](https://huggingface.co/datasets/narly/russian-codexes-bge-m3)
- **Content:** Russian legal codes with BGE-M3 embeddings
- **Size:** Download script automatically detects current file sizes

### 3. Configure Environment

```bash
# Copy environment template
cp analyzer/env.example analyzer/.env

# Edit the .env file and add your OpenRouter API key
# Replace 'your_openrouter_api_key_here' with actual key
```

**Required configuration in `analyzer/.env`:**
```bash
OPENROUTER_API_KEY=your_actual_api_key_here
```

### 4. Launch the Platform

```bash
# Build and start all services (first launch may take 10-15 minutes)
docker-compose up --build -d

# Monitor the startup process
docker-compose logs -f
```

**Note**: The first launch will download the BAAI/bge-m3 model (~2GB) and may take some time.

### 5. Initialize Database and Load Legal Dataset

**Option A: Quick Setup (Recommended)**
```bash
# Load the complete legal dataset (413k+ chunks with embeddings)
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --upload --clear

# For systems with limited RAM, use smaller chunk sizes:
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --upload --clear --csv-chunk-size 50
```

**Option B: Generate Embeddings from Scratch**
Only if you don't have the pre-generated embeddings file:
```bash
# Generate embeddings (takes 1-2 hours)
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --generate

# Upload to database
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --upload --clear
```

### 6. Access the Application

Once setup is complete, access these URLs:

- **ğŸŒ Web Application**: [http://localhost:8080](http://localhost:8080) - Main user interface
- **ğŸ“‹ Backend API**: [http://localhost:8000](http://localhost:8000) - Document processing API
- **ğŸ” AI Analysis API**: [http://localhost:8001](http://localhost:8001) - RAG and analysis API
- **ğŸ“š API Documentation**: [http://localhost:8001/docs](http://localhost:8001/docs) - Interactive API docs

**ğŸ‰ You're ready to analyze legal documents!**

## ğŸ–¥ï¸ User Interface

The platform provides a complete web interface with three main screens:

1. **Upload Screen**: Drag-and-drop interface for document upload
2. **Processing Screen**: Real-time processing status with progress indicators  
3. **Results Screen**: Comprehensive analysis results with risks and recommendations

### Service Architecture
- **Frontend** (Port 8080): Vue.js SPA with upload, processing, and results interfaces
- **Backend** (Port 8000): Spring Boot API handling document uploads and orchestration
- **Analyzer** (Port 8001): FastAPI microservice with RAG pipeline and LLM integration
- **Database** (Port 5432): PostgreSQL with pgvector for vector similarity search

## ğŸ“Š Database Structure

The optimized database structure separates rules metadata from searchable chunks:

### Tables
- **`rules`**: Complete legal rules with metadata (190,000+ entries)
  - `rule_id`, `file`, `rule_number`, `rule_title`, `rule_text`
  - `section_title`, `chapter_title`, `start_char`, `end_char`, `text_length`

- **`rule_chunks`**: Text chunks with embeddings for semantic search (413,000+ entries)
  - `chunk_id`, `rule_id` (foreign key), `chunk_number`, `chunk_text`
  - `chunk_char_start`, `chunk_char_end`, `embedding` (1024-dimensional vector)

- **`analysis_results`**: Document analysis results storage

### Benefits
- **Better Granularity**: Search operates on meaningful text chunks rather than full articles
- **Improved Relevance**: More precise semantic matching with chunk-level embeddings
- **Efficient Storage**: Embeddings only stored for searchable chunks
- **Scalable Design**: Foreign key relationships maintain data integrity

## ğŸ“ Dataset Files

The system uses datasets downloaded from [Hugging Face Hub](https://huggingface.co/datasets/narly/russian-codexes-bge-m3) and stored in the project root `datasets/` directory:

- **`rules_dataset.csv`**: Complete legal rules metadata (190,846 rules)
- **`chunks_dataset.csv`**: Text chunks for embedding (413,453 chunks)  
- **`chunks_with_embeddings.csv`**: Pre-generated BGE-M3 embeddings (413,453 vectors)

**ğŸš€ Easy Download**: Files are automatically downloaded using the provided script:
```bash
python analyzer/scripts/download_datasets.py
```

**ğŸ“Š Dataset Details:**
- **Source Repository**: [narly/russian-codexes-bge-m3](https://huggingface.co/datasets/narly/russian-codexes-bge-m3)
- **Embedding Model**: BAAI/bge-m3 (1024 dimensions)
- **Content**: Russian Civil Codes parsed and chunked for semantic search

## ğŸ“š API Documentation

### Core Endpoints

#### ğŸ” Analyzer API (Port 8001)
- **GET /health**: Health check with database connectivity status
- **POST /api/v1/retrieve-chunk**: Semantic document chunk retrieval
- **POST /api/v1/retrieve-rules**: Semantic retrieval of unique legal rules
- **POST /api/v1/analyze**: Legal document analysis with risk assessment
- **POST /api/v1/embed**: Text embedding generation
- **GET /api/v1/metrics/retrieval**: Embedding quality metrics

#### ğŸ”„ Backend API (Port 8000)
- **POST /api/v1/get_analysis**: Document upload and analysis orchestration
- **GET /api/v1/health**: Service health check

### API Usage Examples

#### Semantic Search
```bash
curl -X POST "http://localhost:8001/api/v1/retrieve-rules" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "contract termination conditions",
    "k": 5,
    "distance_function": "cosine"
  }'
```

#### Document Analysis
```bash
curl -X POST "http://localhost:8001/api/v1/analyze" \
  -F "id=my_contract_123" \
  -F "file=@contract.txt"
```

#### Text Embedding
```bash
curl -X POST "http://localhost:8001/api/v1/embed" \
  -H "Content-Type: application/json" \
  -d '{"text": "Your text to embed"}'
```

## ğŸ”§ Advanced Configuration

### Environment Variables

The `analyzer/.env` file supports these configuration options:

**Required:**
```bash
OPENROUTER_API_KEY=your_openrouter_api_key_here
```

**Database (pre-configured for Docker):**
```bash
POSTGRES_DB=smartclause_analyzer
POSTGRES_USER=smartclause
POSTGRES_PASSWORD=smartclause
DATABASE_URL=postgresql://smartclause:smartclause@postgres:5432/smartclause_analyzer
```

**Model Configuration:**
```bash
OPENROUTER_MODEL=google/gemini-2.5-flash-lite-preview-06-17  # Default LLM
EMBEDDING_MODEL=BAAI/bge-m3                                   # Default embeddings
EMBEDDING_DIMENSION=1024
```

**Performance Settings:**
```bash
MAX_FILE_SIZE=10485760              # 10MB file upload limit
DEFAULT_K=5                         # Default retrieval count
MAX_K=20                           # Maximum retrieval count
MAX_CONCURRENT_THREADS=4           # Concurrent operations
MAX_CONCURRENT_LLM_CALLS=10        # Concurrent LLM calls
MAX_CONCURRENT_EMBEDDINGS=8        # Concurrent embedding generation
```

**Timeout and Retry Settings:**
```bash
LLM_TIMEOUT=90                     # LLM API timeout (seconds)
EMBEDDING_TIMEOUT=15               # Embedding timeout (seconds)
MAX_RETRIES=3                      # Retry attempts
RETRY_DELAY=1.0                    # Initial retry delay
RETRY_BACKOFF_FACTOR=2.0           # Exponential backoff
```

### Dataset Processing Options

The unified script `process_and_upload_datasets.py` provides flexible options:

```bash
# Upload existing embeddings (if file exists)
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --upload --clear

# Generate embeddings from scratch (takes 1-2 hours)
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --generate --upload --clear

# Batch processing with custom batch size
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --generate --batch-size 50

# Non-interactive mode (skip confirmations)
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --upload --clear --no-confirm
```

## ğŸš§ Troubleshooting

### Common Issues

#### Dataset Download Problems
```bash
# Re-download all datasets
python analyzer/scripts/download_datasets.py --force

# Check if datasets exist and verify sizes
ls -lh datasets/

# The script will show actual file sizes when run
# Sizes are automatically detected from remote files

# If download fails, check internet connection and try again
curl -I https://huggingface.co/datasets/narly/russian-codexes-bge-m3/resolve/main/rules_dataset.csv
```

#### Memory Issues During Embedding Generation
```bash
# Use smaller batch sizes
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --generate --batch-size 10

# Monitor memory usage during upload
docker stats

# For very low-memory systems, use even smaller settings
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --upload --clear --csv-chunk-size 250 --batch-size 25

# For embedding generation on low-memory systems, use smaller batch sizes
docker-compose exec analyzer python scripts/process_and_upload_datasets.py --generate --batch-size 10
```

#### Database Connection Issues
```bash
# Check database container status
docker-compose ps postgres

# View database logs
docker-compose logs postgres

# Restart database service
docker-compose restart postgres

# Reset database completely
docker-compose down -v
docker-compose up postgres -d
```

#### Service Startup Issues
```bash
# Check all service logs
docker-compose logs

# Check specific service
docker-compose logs analyzer

# Rebuild containers if needed
docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

#### Missing Dataset Files
```bash
# Verify dataset files exist
ls -la datasets/

# If files are missing, download them:
python analyzer/scripts/download_datasets.py

# Verify download completed successfully
ls -lh datasets/chunks_with_embeddings.csv

# Should show the embeddings file with substantial size
```

#### API Key Issues
```bash
# Verify environment file exists
ls -la analyzer/.env

# Check if API key is properly set
docker-compose exec analyzer env | grep OPENROUTER

# Test API key directly
curl -H "Authorization: Bearer YOUR_API_KEY" https://openrouter.ai/api/v1/models
```

### Performance Optimization

**For faster embedding generation:**
- Use larger batch sizes: `--batch-size 200` (if you have sufficient memory)
- Use a machine with more CPU cores and RAM
- For production: Use GPU-enabled Docker setup with CUDA

**For production deployment:**
- Use external PostgreSQL with more memory and SSD storage
- Implement Redis for caching frequently accessed chunks
- Use a CDN for static assets
- Enable gzip compression
- Set up horizontal scaling with load balancers

## ğŸ” Legal Dataset Details

The platform includes a comprehensive chunked Civil Code dataset:

- **190,846 legal rules** with complete metadata and hierarchical structure
- **413,453 text chunks** with optimized 800-character chunks and 500-character overlap
- **BAAI/bge-m3 embeddings** (1024-dimensional) for high-quality semantic understanding
- **Structured relationships** between rules and their constituent chunks
- **Memory-optimized processing** with batch generation and streaming upload

### Dataset Processing Pipeline
1. **Rules Extraction**: Legal articles parsed from source documents with metadata
2. **Text Chunking**: Rules split into overlapping chunks for comprehensive semantic coverage
3. **Embedding Generation**: Each chunk encoded using BAAI/bge-m3 model with batch processing
4. **Database Upload**: Rules and chunks stored with proper foreign key relationships and indexing

## ğŸ“Š MVP Implementation Status

### âœ… Completed and Ready
- **ğŸŒ Full Web Application** with complete UI workflow and responsive design
- **ğŸ“„ Document Upload & Analysis** with comprehensive file processing pipeline
- **ğŸ—„ï¸ Chunked Vector Database** with 413,000+ text chunks and embeddings
- **ğŸ” Semantic Search** with configurable similarity functions and chunk-level precision
- **ğŸ”— REST API** with comprehensive endpoints and interactive Swagger documentation
- **ğŸ³ Docker Deployment** with full service orchestration and easy setup
- **âš¡ Real-time Processing** with status updates and progress tracking
- **ğŸ›¡ï¸ Error Handling** with comprehensive validation and user feedback
- **ğŸ”§ Unified Data Processing** with flexible embedding generation and upload scripts
- **ğŸ“ˆ Performance Optimization** with concurrent processing and retry mechanisms
- **ğŸŒ Web Deployment** via [SmartClause](http://82.202.138.178:8080/)

### ğŸ¯ Future Enhancements
- **ğŸ‘¤ User Management**: Authentication system and personalized document history
- **ğŸ’¬ Chat Interface**: Interactive legal consultation with conversation history

## ğŸš€ Ready for Production

This MVP provides a solid foundation for a legal tech platform with:

- **ğŸ—ï¸ Scalable Architecture**: Microservices ready for horizontal scaling and load balancing
- **ğŸ”’ Production APIs**: Comprehensive error handling, validation, and security measures
- **âš–ï¸ Legal Domain Expertise**: Purpose-built for Russian legal document analysis with chunk-level precision
- **ğŸ› ï¸ Modern Tech Stack**: Latest frameworks, AI integration patterns, and best practices
- **ğŸ³ Container Deployment**: Consistent environments across development, staging, and production
- **ğŸ¯ Optimized Vector Search**: Chunk-based retrieval for improved relevance and performance
- **ğŸ“š Comprehensive Documentation**: Complete setup guides, API docs, and troubleshooting

**ğŸ‰ Start analyzing legal documents today!** Upload your first document at [http://localhost:8080](http://localhost:8080) after following the Quick Start guide.